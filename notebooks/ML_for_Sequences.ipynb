{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773200ce-8c98-4a16-bd55-b24355950858",
   "metadata": {},
   "source": [
    "We'll start with some magic that will make torch play nice with R on Talapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6066e-b081-4415-ac0d-1b55d7c02455",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sys.setenv(TORCH_HOME='/gpfs/projects/datascience/shared/R/torch/lantern/build/libtorch')\n",
    "libdir='/gpfs/projects/datascience/shared/R/Data4ML'\n",
    ".libPaths(libdir)\n",
    "Sys.setenv(R_LIBS = paste(libdir, Sys.getenv(\"R_LIBS\"), sep=.Platform$path.sep))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aeff51-7407-49d8-b04d-ce501c8f1a2b",
   "metadata": {},
   "source": [
    "# Text Analysis for sequencing\n",
    "\n",
    "We'll be playing around with the DeePromoter [https://github.com/egochao/DeePromoter](https://github.com/egochao/DeePromoter) dataset. This dataset contains 300bp sequences that bracket known transcription start sites (with the TSS located at position 250 out of 300) from the Eukaryotic Promoter Database (EPDnew). Their goal is to accurately predict sequences that act as promoter sites in eukaryotes for subsequent studies. We'll try a few methods to get a feel for utilizing raw text analysis\n",
    "\n",
    "1. Feature engineering and Random Forests\n",
    "\n",
    "2. A Deep Learning Approaching\n",
    "\n",
    "In both cases, we'll need to create sequences to use non-promoter examples. We'll create random sequences, but there are other ways in the literature.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1668686-13d7-4701-a712-889ce06ed3b1",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "We'll use Torch for the dep learning and K-mer for  'feature engineering.'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d151cde-168f-4b2a-866e-b662eaae347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"torch\")\n",
    "library('kmer')\n",
    "library('randomForest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56708e3b-9dad-4af7-8c51-3baa1084d039",
   "metadata": {},
   "source": [
    "# Read in the Sequences File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb46a2-9165-429a-8c30-30e66905c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data<-read.csv(\"/gpfs/projects/datascience/shared/deePromoter_data/hs_pos_TATA.txt\",header=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7606a1f5-54d4-4786-b1ba-1d35b7f1027d",
   "metadata": {},
   "source": [
    "# Data Prep for Structured Features\n",
    "\n",
    "We want to go from our string of letters to structured features. We'll start with getting kmer couts. The package 'kmer' were working with expect our sequencs as a matrix of characters. This means will need to break our strings apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8be48-1532-4b80-9f4e-877772d2c08b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "head(seq_data)\n",
    "pos_char<-strsplit(seq_data[,1],\"\")\n",
    "head(pos_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b734b58-9914-4f5c-922e-bd0c43753d1c",
   "metadata": {},
   "source": [
    "Next were going to use the R command \n",
    "```R\n",
    "sample\n",
    "```\n",
    "we'll use this to generate two kinds of non-promoter sequences\n",
    "1. A random shuffle of our promoter sequences\n",
    "2. Truly random sequences of As, Ts, Gs, and Cs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552a8d6-531b-4a41-b7f3-8eb9263d6b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "n_neg=nrow(seq_data)\n",
    "neg_char=c()\n",
    "\n",
    "for( i in 1:n_neg){\n",
    "    pos_sample=sample(length(pos_char),1)\n",
    "    neg_sample=sample(pos_char[[pos_sample]],length(pos_char[[pos_sample]]))    \n",
    "    neg_char=rbind(neg_char,neg_sample)\n",
    "                   }  \n",
    "\n",
    "rand_char=c()\n",
    "for( i in 1:n_neg){\n",
    "    r_sample=sample(c('A','T','G','C'),300,replace=T)\n",
    "    rand_char<-rbind(rand_char,r_sample)\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45985c3d-c1f0-46e0-ad7f-47718c0637d3",
   "metadata": {},
   "source": [
    "Now we'll create our kmer counts, combine our data and add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e3608b-0eb2-4033-a75d-93c9024e5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Kmers\n",
    "pos=data.frame(kcount(pos_char,k=4))\n",
    "neg=data.frame(kcount(neg_char,k=4))\n",
    "rand=data.frame(kcount(rand_char,k=4))\n",
    "\n",
    "\n",
    "#Combine and add a Label\n",
    "dataset=rbind(pos,neg,rand)\n",
    "dataset$y <- c(rep(1,nrow(pos)),rep(0,nrow(neg)+nrow(rand)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20cb43f-12e5-4b7f-bc8c-49457fd7392f",
   "metadata": {},
   "source": [
    "head(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40328207-6617-4ac1-a023-a0587c8301cf",
   "metadata": {},
   "source": [
    "# Remember from before we'll want to split the dataset into Train and Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910bac66-0ccb-4e2a-aab7-bf2d372eaa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly Select training indices\n",
    "train_index <- sample(seq_len(nrow(dataset)),size= nrow(dataset)*.8, replace=FALSE )\n",
    "\n",
    "#Split the data\n",
    "train <- dataset[train_index,]\n",
    "test <- dataset[-train_index,]\n",
    "\n",
    "\n",
    "x<-subset(train, select=-y)\n",
    "y<-as.factor(train$y)\n",
    "\n",
    "xtest<-subset(test, select=-y)\n",
    "ytest<-as.factor(test$y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d1c16-296e-48e8-9b8e-1f728d83bb4d",
   "metadata": {},
   "source": [
    "# Fit a Random Forest\n",
    "\n",
    "just like last time we can use a random forest and importance to learning what kmers might be useful for predicting promoters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f97a59-6046-40a4-8d89-0f86e9c7a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=randomForest(x,y,xtest,ytest,ntree=100,importance=TRUE,keep.forest=TRUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e031a2-0787-442f-b6e9-c875d2a19afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(rf)\n",
    "print(rf)\n",
    "importance(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07372c0-8bc0-4339-bee2-0dd535817fc7",
   "metadata": {},
   "source": [
    "## The importance data is big so let's sort it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57064530-c096-466f-9a40-0903231a8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp <- as.data.frame(importance(rf))\n",
    "imp[order(imp$MeanDecreaseAccuracy,decreasing = T),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4c758-f176-4eee-ab46-2e415b7caca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best=which.max(imp$MeanDecreaseAccuracy)\n",
    "#best=which(rownames(imp)=='TTTT')\n",
    "name=rownames(imp)[best]\n",
    "\n",
    "\n",
    "options(repr.plot.width=10,repr.plot.height=10)\n",
    "ax<-0:30\n",
    "prom<-hist(x[y==1,best],plot=FALSE,breaks=ax)\n",
    "rand<-hist(x[y==0,best],plot=FALSE,breaks=ax)\n",
    "\n",
    "c1=rgb(1,.1,.2,alpha=.30)\n",
    "c2=rgb(1,1.,.2,alpha=.30)\n",
    "\n",
    "plot(prom,col=c2,xlab=name,add=F,freq=FALSE)\n",
    "plot(rand, col=c1,add=T,freq=FALSE)\n",
    "\n",
    "legend(5,0.5,legend=c('Promoters','Random'),fill=c(c2,c1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea7cd1-e968-4e27-aa2f-b2cfcd572efb",
   "metadata": {},
   "source": [
    "# Exercise try to predict new data\n",
    "Let's see how well our model works on a new dataset\n",
    "1. Process the datasets below into kmers\n",
    "2. Make a histogram comparing the output of this dataset to random sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ffb605-1998-487d-8d5a-629b2a02c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_data<-read.csv(\"/gpfs/projects/datascience/shared/deePromoter_data/mm_pos_TATA.txt\",header=FALSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09251f2-6af8-486b-9cda-26ae916871cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Your Code Here\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a1d053-5895-4137-8023-81c47d3954fa",
   "metadata": {},
   "source": [
    "# Try with a Deep Neural Network\n",
    "\n",
    "The other way we can look at sequence data is with deep learning. This can be a lot more involve, but sometimes the payoff is worth it.\n",
    "\n",
    "1. Instead of k-mers we're going to use the sequences mainly as is, however, we need to encode each character with a number. We'll use A=1,T=2,G=3,C=4. \n",
    "\n",
    "2. We'll need to write a deep learning model in torch. This can be a creative process with chances to improve your results, we'll use a Convolutional Neural Network or CNN.\n",
    "\n",
    "3. We'll have to write a training loop, unlike our random forest we'll need to do this ourseleves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a8a67-35df-4163-9855-7c648f4dbb1c",
   "metadata": {},
   "source": [
    "# Convert our Data to Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f5a2c4-6200-4b31-9587-03d7400694dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_int <-function(x){\n",
    "    out=c()\n",
    "    for (i in 1:length(x)){\n",
    "    if (x[i]=='A'){co=1}\n",
    "    if (x[i]=='T'){co=2}\n",
    "    if (x[i]=='G'){co=3}\n",
    "    if (x[i]=='C'){co=4}\n",
    "    out=c(out,co)\n",
    "    }\n",
    "return(out)\n",
    "}\n",
    "\n",
    "\n",
    "pos_mat <- do.call(\"rbind\",pos_char)\n",
    "\n",
    "all_char=rbind(pos_mat,neg_char,rand_char)\n",
    "head(all_char)\n",
    "\n",
    "all_char=apply(all_char,2,to_int)\n",
    "head(all_char)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7318d8d2-91a4-458d-99e5-093d280bbfd8",
   "metadata": {},
   "source": [
    "## Split the data like before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c894037-4f2b-4df1-8712-9c5148f71667",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_train <- all_char[train_index,]\n",
    "dnn_test <- all_char[-train_index,]\n",
    "#print(nrow(dnn_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85c054-ad3e-4280-8a0f-1bdc5e5799f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(all_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524838e3-468a-4b2d-9473-807217fdb3c0",
   "metadata": {},
   "source": [
    "# Our DNN Model\n",
    "There is a flow to most models in torch. \n",
    "1. Create your layers\n",
    "\n",
    "   * **embedding** All text models will start with an 'Embedding' layer which maps each of our input integers into a a continous vector. It's just a learned matrix that returns the row of the each integer you put in\n",
    "   * Intermediate layers - Convolutions, LSTM, etc\n",
    "   * An activation function - This is applied at every layer it makes the model non-linear\n",
    "   * Classification head - Fully connected layer with an outputsize of one\n",
    "\n",
    "2. Define how they are put together to go from inputs to outputs ( this is called the foward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dedd7a-e0a1-49ef-8647-dbf9ee3de59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Deep Neural Network has two functions initializtion and forward\n",
    "\n",
    "net <- nn_module(\n",
    "  \"CNN model\",\n",
    "\n",
    "  initialize = function(vocab_size,emb_dim,\n",
    "                        fc1_dim,\n",
    "                        fc2_dim) {\n",
    "    self$embedder <- nn_embedding(4,emb_dim)\n",
    "\n",
    "    self$conv1<- nn_conv1d(emb_dim,50,10,stride=10)\n",
    "    self$conv2<- nn_conv1d(50,50,10,stride=10)\n",
    "\n",
    "    self$fc1 <- nn_linear(150, fc1_dim)\n",
    "    self$fc2 <- nn_linear(fc1_dim, fc2_dim)  \n",
    "    self$drop <- nn_dropout(0.2)\n",
    "    self$output <- nn_linear(fc2_dim, 1)\n",
    "    self$act <- nn_relu()\n",
    "  },\n",
    "\n",
    "  forward = function(x) {\n",
    "    x <- self$embedder(x)\n",
    "\n",
    "    x <- self$act(self$conv1(x$permute(c(1,3,2))))\n",
    "    x<-self$drop(x)\n",
    "    x <- self$act(self$conv2(x))\n",
    "    x<-self$drop(x)\n",
    "    x<-torch_flatten(x,start_dim=2)\n",
    "    x<-self$act(self$fc1(x))\n",
    "    x<-self$drop(x)\n",
    "    x<-self$act(self$fc2(x))\n",
    "    x<-self$drop(x)\n",
    "    x<-self$output(x)\n",
    "    x=torch_sigmoid(x)\n",
    "    x=torch_squeeze(x)\n",
    "    return(x)\n",
    "  }\n",
    ")\n",
    "\n",
    "model=net(4,4,10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3268e278-0c7c-49e9-b651-79b051a876f2",
   "metadata": {},
   "source": [
    "# Batches \n",
    "DNN operate one batch at a time. A batch is just a fixed number of examples how many is refered to as the **batch_size**. **batch_sizes** vary from a few to hundreds or thosands 32 is a common default.\n",
    "\n",
    "**Why Batches**? Computational requirments on DNN are high, normally too high to run an entire dataset at once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59780053-1834-4647-b240-c3e6225952b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "evaluate <- function(X,Y,batch_size){\n",
    "    x_torch=torch_tensor(X,dtype=torch_int())\n",
    "    y_torch=torch_tensor(Y,dtype=torch_float())-1 #here labels must be 0 or one but factors convert to 1/2\n",
    "    dnn_pred=c()\n",
    "    \n",
    "    num_batch=floor(nrow(X)/batch_size)\n",
    "    loss_v=c()\n",
    "    print('Eval')\n",
    "    model$eval()\n",
    "    for (i in 0:num_batch ){\n",
    "        end = (i+1)*batch_size\n",
    "        if (end >nrow(X)){\n",
    "            end=nrow(X)\n",
    "            }\n",
    "        batch_i=torch_tensor((i*batch_size+1):end)\n",
    "        batch=x_torch[batch_i,]\n",
    "        output=torch_squeeze(model(batch))\n",
    "        loss <- criterion(output,y_torch[batch_i]) \n",
    "        loss_v=c(loss_v,c(loss$item()))\n",
    "        dnn_pred<-c(dnn_pred,as.array(output))\n",
    "        flush.console()    \n",
    "    }\n",
    "    model$train()\n",
    "    \n",
    "    return( list(pred=dnn_pred,loss=loss_v))\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06c870-4afd-45d9-b8a7-b83c5b6cfeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer <- optim_adam(model$parameters,lr=1e-3)\n",
    "criterion <- nn_bce_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c63b90-3bcd-4bf6-92d0-7b5a6228f23b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size <- 10\n",
    "num_epochs <- 4\n",
    "\n",
    "\n",
    "num_data_points <- length(y)\n",
    "num_batches <- floor(num_data_points/batch_size)\n",
    "\n",
    "\n",
    "for(current_epoch in 1:num_epochs){\n",
    "  permute <- sample(num_data_points)\n",
    "  data_x <- torch_tensor(dnn_train[permute,],dtype=torch_int())\n",
    "  data_y <- torch_tensor(y[permute],,dtype=torch_float())-1\n",
    "  \n",
    "  running_loss=c()\n",
    "  for(batch_idx in 1:num_batches){\n",
    "    batch_i <- (batch_size*(batch_idx-1) + 1):(batch_idx*batch_size)\n",
    "    batch=data_x[batch_i,]\n",
    "    optimizer$zero_grad()\n",
    "    output <- model(batch)\n",
    "    loss <- criterion(output, data_y[batch_i]) #here labels must be 0 or one but factors convert to 1/2\n",
    "    loss$backward()\n",
    "    optimizer$step()\n",
    "\n",
    "    running_loss=c(running_loss,c(loss$item()))\n",
    "      \n",
    "    if (length(running_loss)==50) {\n",
    "        cat(\"train_loss\",mean(running_loss), \"epoch\",current_epoch,\"batch\",batch_idx,\"\\n\")\n",
    "        running_loss=c()\n",
    "        flush.console()\n",
    "        }\n",
    "    }\n",
    "  ev_vals=evaluate(dnn_test,ytest,10)\n",
    "  cat(\"Validation Loss\",mean(ev_vals$loss),\"Epoch\",current_epoch,\"\\n\")\n",
    "      \n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf43a48-ab62-4f54-a3ee-3cb7f2261fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model$eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb7e7b3-693d-4937-b015-8c03c885686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_save(model$state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2088cfc5-cfff-4a2f-82f5-fe2c58b6d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model$load_state_dict(torch_load('/projects/datascience/shared/DATA4ML_weights/model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed2c401-9e22-4f7e-b29d-052ca3afbbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ev_vals=evaluate(dnn_test,ytest,10)\n",
    "dnn_pred=ev_vals[[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2191d-5543-449b-aacd-865230efa6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(((dnn_pred>0.5) == (ytest==1)))/length(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed8e51-6438-41ed-b1f4-90b28c12d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(dnn_pred[ytest==1]>0.5)/sum(ytest==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10403e67-10e7-413d-90cb-b1d16ab2dfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "options(repr.plot.width=10,repr.plot.height=10)\n",
    "ax<-pretty(0:1,n=50)\n",
    "prom<-hist(dnn_pred[ytest==1],plot=FALSE,breaks=ax)\n",
    "rand<-hist(dnn_pred[ytest==0],plot=FALSE,breaks=ax)\n",
    "\n",
    "c1=rgb(1,.1,.2,alpha=.30)\n",
    "c2=rgb(1,1.,.2,alpha=.30)\n",
    "\n",
    "plot(prom,col=c2,xlab='P(Prom|X)',freq=F)\n",
    "plot(rand, col=c1,add=TRUE,freq=F)\n",
    "legend(200,600,legend=c('Promoters','Random'),fill=c(c2,c1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e747454-3e53-465d-b7f4-30ed360f2748",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "Try our DNN on Mouse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e66280e-a5b2-427f-b9c8-e61f7ce72b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Your Code Here\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
